---
title: "Introduction to statistics"
subtitle: "Biomedical Engineering - academic year 2023-2024"
format: 
  inrae-revealjs:
    footer: ""
    chalkboard: true
    fontsize: 2em
author: Maud Delattre 
execute:
  echo: false
---

```{r}
#| label: loadlibrary
library(pastecs)
library(ggplot2)
library(cowplot)
library(tidyverse)
library(medicaldata)
```

## Topics covered in this course

-   Descriptive statistics

-   Basics of probability

-   Estimation

-   Confidence intervals

-   Tests

-   Advanced exploratory data analysis tools

## What will you learn in this course?

-   [**Data exploration:**]{style="color: blue"} describe and understand the structure of a dataset.

-   [**Modelling:**]{style="color: blue"} move from a biological question to a statistical model accounting for the nature of the data and sources of variability.

-   [**Statistical inference:**]{style="color: blue"} get some informations about the (unknown) parameters of the model:

    -   estimating the parameter,

    -   give a (restricted) range of possible values for the parameter,

    -   decide whether the parameter belongs to a given interval or not.

-   [**R programming:**]{style="color: blue"} data analysis.

# First session: descriptive statistics

## Objective

Learn how to use figures and graphs to

. . .

1.  [Describe a sample]{style="color: blue"}

    -   *"How are individuals distributed w.r.t. height? To age?"*

    -   *"How are individuals distributed between male and female?"*

. . .

2.  [Evaluate the relationship between descriptors]{style="color: blue"}

    -   *"How do height and age seem to be related?"*

    -   *"Does the height distribution look the same for male and female?"*

# Definitions

## Population vs sample

-   [**Population**]{style="color: blue"}: entire group that you want to draw conclusions about (not always people)

-   [**Sample**]{style="color: blue"}: often a smaller, manageable version of a larger group (population of interest)

-   [**Unit/individual**]{style="color: blue"}: observed member of the population of interest

[Why working on samples rather than the entire population of interest?]{style="color: red"}

. . .

-   the population size is too large

-   data collection is costly

-   $\ldots$ time consuming

-   $\ldots$ and sometimes destructive

## Variables

[**Variable**]{style="color: blue"}: a descriptor, *i.e.* one characteristic measured on the sample

. . .

-   [**Quantitative/ numerical variables**]{style="color: blue"}: characteristics that can be counted or measured

    -   **Continuous**: variables that can take an infinite number of real values within a given interval

    -   **Discrete**: variables that can only take a finite number of real values within a given interval

-   [**Qualitative/ categorical variables**]{style="color: blue"}: values corresponding to categories (levels)

    -   **Nominal**: levels without natural order

    -   **Ordinal**: order relation between levels

::: callout-important
The distinction between categorical and quantitative variables is important because they are analyzed differently.
:::

## Raw data tables

The values taken by the variables on [$n$]{style="color: red"} individuals are presented in a table :

$$\left[\begin{array}{cccc}
 x_{1} & y_{1} & \ldots & z_{1} \\
 x_{2} & y_{2} & \ldots & z_{2} \\
 \vdots & \vdots & \vdots & \vdots \\
 x_{n} & y_{n} & \ldots & z_{n}
\end{array}\right]$$

-   [$n$]{style="color: red"} is the sample size.

-   Each line (resp. column) represents an individual (resp. a variable).

-   The sequence of values taken by a variable on the observed individuals is called a [**statistical series**]{style="color: blue"}. The statistical series for variable $X$ is denoted by $x_1,x_2,\ldots,x_n$.

## Example

[**Data**]{style="color:blue"}

A data subset of **licorice_gargle**, a **R** dataset from a study enrolling adult patients undergoing elective thoracic surgery. <!-- Étude portant sur des patients adultes subissant une chirurgie thoracique élective nécessitant un tube endotrachéal à double lumière. -->

. . .

```{r}
#| label: load-data-example
data <- licorice_gargle %>% 
  mutate(preOp_gender=as.factor(preOp_gender)) %>%
  mutate(intraOp_surgerySize=as.factor(intraOp_surgerySize)) %>%
  rename(gender=preOp_gender,BMI=preOp_calcBMI,age=preOp_age,surgerySize=intraOp_surgerySize) %>%
  select(gender,BMI,age,surgerySize)
```

```{r}
#| label: show-data-example
#| echo: true
str(data)
head(data)
```

[**Questions**]{style="color: blue"}

-   Sample size?

-   Nature of the variables?

# Univariate descriptive statistics

## Examining qualitative variables (1/3)

Example: *surgery size*

[**1-**]{style="color: red"} The statistical series can be summarized into a [**frequency table**]{style="color: blue"} showing **counts within each category**

```{r}
#| label: desc-surgerysize-frequency-raw
#| echo: true
table(data$surgerySize)
```

or showing **proportions within each category**

```{r}
#| label: desc-surgerysize-frequency-percentage
#| echo: true
prop.table(table(data$surgerySize))
```

. . .

[**2-**]{style="color: red"} The [**mode**]{style="color: blue"} of the series is the most frequently observed value.

<!-- ```{r} -->

<!-- #| label: create-mode-function  -->

<!-- getmode <- function(v) { -->

<!--   uniqv <- unique(v) -->

<!--   uniqv[which.max(tabulate(match(v, uniqv)))] -->

<!-- } -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| label: desc-surgerysize-mode -->

<!-- #| echo: true -->

<!-- getmode(data$surgerySize) -->

<!-- ``` -->

. . .

Remark: An observed distribution may have several modes. <!-- ::: callout-important --> <!-- An observed distribution may have several modes. --> <!-- ::: -->

## Examining qualitative variables (2/3)

[**Bar plot**]{style="color: blue"}: height of the bars proportional to the **numbers** or **proportions** in each category of the variable

```{r}
#| label: desc-surgerysize-barplot
#| echo: true
#| fig.asp: 0.2
ggplot(data, aes(x=surgerySize)) + geom_bar() + xlab("Surgery size")

data.surgery <- data %>% count(surgerySize) %>% 
  mutate(perc = n / nrow(data)) 
ggplot(data.surgery, aes(x = surgerySize, y = perc)) + geom_bar(stat = "identity")
```

## Examining qualitative variables (3/3)

[**Pie chart**]{style="color: blue"}: piece areas are proportional to the **proportions** in each category of the variable

```{r}
#| label: desc-surgerysize-piechart
#| echo: true
ggplot(data.surgery, aes(x = "", y = perc,fill=surgerySize)) +
geom_bar(stat = "identity",width=1) + coord_polar("y", start=0)
```

## Examining quantitative variables (1/9)

-   [**Centrality measures**]{style="color: blue"} that tells us about how a typical observation looks like.

-   [**Measures of dispersion**]{style="color: blue"} that tell how observations are spread out around this typical individual.

## Examining quantitative variables (2/9)

The [**mean (arithmetic)**]{style="color: blue"} of a series $\{x_i,i=1,\ldots,n\}$ is defined as: $$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$

::: callout-caution
What you need to remember about the mean:

-   each series has only one mean,

-   it is rarely a value from the series,

-   it is susceptible to outliers.
:::

**Property** :

Define the new series $y$ as: $y=ax+b$.

What is the formula of the arithmetic mean of the series $y$ as a function of $a$, $b$ and $\bar{x}$?

## Examining quantitative variables (3/9)

-   [**Mode**]{style="color: blue"}

-   [**Median**]{style="color: blue"} $x_{1/2}$: {Nb of values $\geq x_{1/2}$} $=$ {Nb of values $\leq x_{1/2}$ }

-   [**Quantile of order** $p$, $x_p$]{style="color: blue"}: value such that a proportion $p$ of the observations is less than $x_p$.

*Example: BMI*

```{r}
#| label: desc-BMI-statdesc
#| echo: true
summary(data$BMI)
```

*Example: age*

```{r}
#| label: desc-age-statdesc
#| echo: true
summary(data$age)
```

## Examining quantitative variables (4/9)

[**1-**]{style="color: red"} [**Variance**]{style="color: blue"}: measure of the dispersion of the series around the mean

-   *empirical* variance: $$
         s_{emp,x}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 = \bar{x^2} - (\bar{x})^2
        $$

-   *corrected* variance: $$
         s_{corr,x}^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2% =  \frac{1}{n-1} \sum_{i=1}^{n} x_i^2 - \frac{n}{n-1} (\bar{x})^2
        $$

**Property**:

Define the new series $y$ as: $y=ax+b$.

What is the formula of the variance of the series $y$ as a function of $a$, $b$ and $s^2_{x}$?

[**2-**]{style="color: red"} [**Standard deviation**]{style="color: blue"}: square root of the variance; either $s_{emp,x} = \sqrt{s_{emp,x}^2}$ or $s_{corr,x} = \sqrt{ s_{corr,x}^2}$

## Examining quantitative variables (5/9)

-   [**Coefficient of variation**]{style="color: blue"}: $$
    CV = \frac{s_x}{\bar{x}}
    $$Unit?

-   [**Range**]{style="color: blue"}: difference between the largest and the smallest value of the series: $$
    E = x_{(n)} - x_{(1)}
    $$ Remark: susceptible to outliers

-   [**Inter-quartile range**]{style="color: blue"}: difference between third and first quartiles of the series: $$
    E_{IQ} = x_{3/4} - x_{1/4}
    $$

## Examining quantitative variables (6/9)

*Example: age*

```{r}
#| label: desc-age-statdesc-2
#| echo: true
stat.desc(data$age)
```

*Example: BMI*

```{r}
#| label: desc-BMI-statdesc-2
#| echo: true
stat.desc(data$BMI)
```

## Examining quantitative variables (7/9)

Representation of the series distribution via [**histograms**]{style="color: blue"}

```{r}
ggplot(data, aes(x=age)) + geom_histogram() + xlab("Age")
```

## Examining quantitative variables (8/9)

or via [**boxplots**]{style="color: blue"}

![Boxplots](./Pictures/Construction_boxplot.png)

## Quantitative variables (9/9)

*Example: age*

```{r}
ggplot(data, aes(x=age)) + coord_flip() +geom_boxplot()
```

# Bivariate descriptive statistics

## Between quantitative variables (1/3)

The [**scatter plot**]{style="color: blue"} is used to represent two quantitative variables simultaneously and have an idea of the nature of the relationship between them.

*Example*

```{r}
ggplot(data, aes(x=age, y=BMI)) + geom_point()
```

## Between quantitative variables (2/3)

The [**correlation coefficient**]{style="color: blue"} is a measure of **linear** relationship between two quantitative variables.

**Definition** $$
r = \frac{s_{xy}}{s_x s_y}
$$ where $s_x$ and $s_y$: standard deviations of $x$ and $y$, and $s_{xy}$: covariance between $x$ and $y$, \textit{i.e.}: $$
 s_{xy} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})
 $$

. . .

*Example*

```{r}
#| label: correlation coefficient
#| echo: true
cor(data$age,data$BMI)
```

## Properties and interpretation of the correlation coefficient

-   $-1<r<1$,

-   if $r<0$: negative/decreasing linear relationship,

-   if $r>0$: positive/increasing linear relationship,

-   if $|r|\approx1$: very strong linear relationship,

-   if $r=0$: no **linear** relationship.

## Between qualitative variables

The [**contingency table**]{style="color: blue"} is a table showing the frequencies of one variable in rows and another in columns.

```{r}
#| label: contingency-table-prep
levels(data$surgerySize) <- c("Small","Medium","Large")
levels(data$gender) <- c("Male","Female")
```

```{r}
#| label: contingency-table
#| echo: true
tab <- table(data$gender,data$surgerySize)
rownames(tab) <- levels(data$gender)
colnames(tab) <- levels(data$surgerySize)
tab
```

. . .

```{r}
#| label: contingency-table-2
#| echo: true
proportions(tab)
```

. . .

```{r}
#| label: contingency-table-3
#| echo: true
proportions(tab, margin=1)
```

. . .

```{r}
#| label: contingency-table-4
#| echo: true
proportions(tab, margin=2)
```

## Take home message

-   Describing the data at hand is an essential step prior to any statistical study.

-   It provides some data summaries and can be used to highlight patterns in the data.

-   We have seen univariate and bivariate descriptive statistics tools.

-   Different tools are used depending on the nature of the variables.

# Second session: some notions of probability

## Why this probability class?

-   Kind of [toolbox]{style="color: red"} for the rest of the statistics course

    -   Statistical models

    -   Estimators, confidence intervals and tests

-   Introduce essential concepts and definitions (*e.g.* [random variable]{style="color: blue"}, [realization]{style="color: blue"} of a random variable)

-   Introduce the [key probability distributions]{style="color: blue"} (Gaussian, Fisher, Student, Chi-square)

# Preliminary definitions

## Random experiment

-   A [random experiment]{style="color: blue"} is one

    -   in which the result [cannot be (completely) predicted]{style="color: red"} in advance,

    -   if repeated several times [under identical conditions]{style="color: red"}, it can give rise to [different results]{style="color: red"}.

-   Some examples?

. . .

-   Throwing a die

-   Flipping a coin

-   Assessing whether a patient treated with a new drug will recover or not (or will develop side effects or not)

-   Evaluating, at the time of diagnosis of a disease, what will be the evolution of the patient's symptoms

-   ...

## Random variable

-   An abstract way to talk about the outcomes of random experiments

-   Random variable ($X$):

    -   [*process that relates the random experiment to a value*]{style="color: red"}

    -   characteristic that is beeing measured in the random experiment

    -   something whose value cannot be known in advance

-   Examples ...

## Realization of a random variable

-   [Realization]{style="color:red"} (eq. [observation]{style="color: red"}) :

    -   value obtained for the characteristic (random variable) of interest $X$ at the end of a random experiment

    -   what the experimenter actually observes

-   Note that you cannot observe a random variable $X$ itself ...

-   Examples ...

## Domain

-   The [domain]{style="color: red"} of a random variable $X$ is the set of values that $X$ can take after the experiment has been carried out (set of possible values for $X$).

-   The domain of a random variable $X$ is often denoted $D_X$.

-   Examples ...

. . .

-   [Remark:]{style="color: blue"} $D_X$ is different depending on the nature of $X$.

    -   [Qualitative r.v.:]{style="color: blue"} discrete set corresponding to the set of $X$'s possible modalities

    -   [Discrete quantitative r.v.:]{style="color: blue"} (finite or infinite) discrete set of numerical values

    -   [Continuous quantitative r.v.:]{style="color: blue"} open or closed interval (*e.g.* the set of real numbers, the set of positive real numbers, the set of real numbers between 0 and 1).

# Characterizing random variables

## Distribution of a random variable

-   The [(probability) distribution]{style="color: red"} of r.v. $X$ indicates how the values of the different individuals in the population under study are distributed (do some realizations occur more or less often than others? ).

-   It is defined differently depending on whether $X$ is discrete or continuous.

## P.d. of a discrete r.v.

-   For discrete r.v., we can enumerate all of its possible realizations and associate a specific probability with each possible realization.

-   [Definition:]{style="color: blue"} Let $J$ be the size of the domain of definition of $X$, $a_1$, $a_2$, $\ldots$, $a_J$ be the possible values of $X$ and $p_j = P(X = a_j)$, $j = 1,\ldots,J$ be the set of the probabilities associated with the different values in $D_X$. Then, the probability distribution of $X$ is the set [$\{(a_j ; p_j)\}_{j = 1,\ldots,J}$]{style="color: red"}

-   ✏️ Examples

. . .

-   [Properties:]{style="color: blue"}

    1.  $P(X=x) \in [0,1]$ for any $x \in D_X$

    2.  $\sum_{j=1}^{J} p_j = 1$

## P.d. of a continuous r.v.

-   Impossible to list all the possible realizations of a continuous r.v.

-   Impossible to associate a specific probability with each possible realization

    $\Rightarrow$ We need a different concept to characterize the distribution of the r.v.

. . .

The [density function]{style="color:red"} of a continuous r.v. $X$ is the function $f(\cdot)$ that associates a [probability with each range]{style="color:red"} of realizations of $X$.

. . .

-   [Properties:]{style="color:blue"}

    1.  $f(x) \geq 0$ for any $x \in D_X$

    2.  $P(a<X<b) = \int_{a}^b f(x)dx \geq 0$ for any $a, b \in D_X$ s.t. $a<b$

    3.  $\int_{x \in D_X}^{} f(x)dx = 1$

## P.d. of a continuous r.v.

✏️ Graphical examples and different interpretations as area under the curve

## Cumulative distribution function

-   ⚠️ [For quantitative r.v.]{style="color:red"}

-   [Definitions]{style="color:blue"}

    -   The [cumulative distribution function]{style="color: red"} (c.d.f.) of r.v. $X$ is the function $F_X(\cdot)$ defined as $$
        F_X(x) = P(X \leq x), \; x \in \mathbf{R}
        $$

    -   The [quantile]{style="color: red"} of order $p$ ($0 \leq p \leq 1$) of a r.v. $X$ is the value $x_p$ s.t. $$
        P(X \leq x_p) = p,
        $$ *i.e.*, s.t. $$
        F_X(x_p) = p.
        $$

# Cumulative distribution function: properties

(✏️ Graphical illustration of the following properties)

1.  A c.d.f. is defined on $\mathbf{R}$ and has values in $[0; 1]$: it defines a probability !

2.  A distribution function is an increasing function (in the broad sense).

3.  $\lim_x \rightarrow -\infty \; F_X(x) = 0$ and $\lim_x \rightarrow +\infty \; F_X(x) = 1$

4.  $P(a < X < b) = F_X(b) - F_X(a)$

5.  [If the distribution of $X$ is symmetric about $0$]{style="color: red"}: $F_X(-x) = 1-F_X(x)$ for any $x \in \mathbf{R}$

6.  If $X$ is a continuous r.v.: $$
     f_X(x) = F_X^{' }(x), \; x \in \mathbf{R} 
     $$

    $$
     F_X(x) = \int_{-\infty}^{x} f_X(x) dx
     $$

# Expected value and variance of a quantitative r.v.

## Expected value: definition

-   Central tendency characteristics of a distribution.

-   The [expectated value]{style="color: red"} (or "theoretical" mean) of a r.v. $X$ is the value taken as an average by $X$. It is given by:

    -   if $X$ is a discrete r.v.:

        $$
         E(X) = \sum_{j=1}^{J} a_j p_j = \sum_{j=1}^{J} a_j P(X=a_j)
         $$

    -   if $X$ is a continuous r.v.: $$
         E(X) = \int_{D_X} x f_X(x)dx
         $$

## Expected value: rules

Let $a \in \mathbf{R}$ be a constant and $X$ ($X_1$, $X_2$) any quantitative r.v.

1.  $E(a) = a$

2.  $E(aX) = aE(X)$

3.  $E(X_1 + X_2) = E(X_1) + E(X_2)$

4.  $E(a+X) = a+E(X)$

**📝 Exercise:** $E(X-E(X)) = ?$

## Variance: definition

-   Quantifies the dispersion of the values taken by a r.v. $X$ its "theoretical mean".

    -   A large variance indicates an important dispersion.

    -   A zero variance reveals that $X$ is non random.

. . .

[Definition]{style="color:red"}

$$
V(X) =E\left[ (X-E(X))^2 \right] = E(X^2) - E(X)^2
$$

. . .

Calculation

-   if $X$ is a discrete r.v. : $V(X) = \sum_{j=1}^{J} (a_j-\mu)^2 p_j$

-   if $X$ is a continuous r.v. : $V(X) = \int_{D_X}^{} (x-\mu)^2 f_X(x) dx$

where $\mu=E(X)$.

## Variance: rules

Let $a \in \mathbf{R}$ be a constant and $X$ ($X_1$, $X_2$) any quantitative r.v.

1.  $V(a) = 0$

2.  $V(aX) = a^2 V(X)$

3.  $V(a+X) = V(X)$

4.  $V(X_1+X_2)= \ldots$ (It depends!)

## Remarks

-   The [standard deviation]{style="\"color:red"} of a (quantitative) r.v. $X$ is the square-root of its variance ($\sqrt{V(X)}$).

-   📝 Exercise (centering and standardizing) : Let $X$ be a r.v. with mean $\mu$ and variance $\sigma^2$ and define the transformed variable $$Y = \frac{X-\mu}{\sigma}$$ What are the expected value and the variance of $Y$?

# Some common distributions

## Bernoulli distribution

To describe an experiment whose result can take only two values, called by convention, success or failure.

*Ex.: a candidate passes or fails an examination, whether a sick patient is cured or not,* $\ldots$

[$$X \sim \mathcal{B}(p), \, 0 \leq p \leq 1$$]{style="color:red"}

::: columns
::: {.column width="50%"}
-   Domain: $D_X = \{0,1\}$

-   Probability distribution: $P(X=0)=1-p$ , $P(X=1)=p$
:::

::: {.column width="50%"}
-   Expectation: $E(X)=p$

-   Variance: $V(X)=p(1-p)$
:::
:::

## Binomial distribution

To describe an experiment which consists in counting the number of successes after performing [$n$ independent Bernoulli trials with the same probability $p$]{style="color:red"}.

[$$X \sim \mathcal{B}(n,p), \, n \in \mathbf{N}, \, 0 \leq p \leq 1$$]{style="color:red"}

::: columns
::: {.column width="50%"}
-   Domain: $D_X = \{0,1,\ldots,n\}$

-   Probability distribution: $P(X=k)=\frac{k!}{n! (n-k)!}p^k(1-p)^{n-k}$
:::

::: {.column width="50%"}
-   Expectation: $E(X)=n p$

-   Variance: $V(X)=n p(1-p)$
:::
:::

::: {#fig-binom layout-ncol="3"}
![B(10,0.2)](./Pictures/binom_n10_p02.png){#fig-B1 height="200" width="200" fig-align="center"}

![B(10,0.5)](./Pictures/binom_n10_p05.png){#fig-B2 height="200" width="200" fig-align="center"}

![B(20,0.2)](./Pictures/binom_n20_p02.png){#fig-B3 height="200" width="200" fig-align="center"}

Binomial distributions
:::

## Poisson distribution

To describe counts.

[$$X \sim \mathcal{P}(\lambda), \lambda >0$$]{style="color:red"}

::: columns
::: {.column width="50%"}
-   Domain: $D_X = \mathbf{N} = \{0,1,\ldots\}$

-   Probability distribution: $P(X=k)=e^{-\lambda} \frac{\lambda^k}{k!}$
:::

::: {.column width="50%"}
-   Expected value: $E(X)=\lambda$

-   Variance: $V(X)=\lambda$
:::
:::

::: {#fig-pois layout-ncol="3"}
![P(1.5)](./Pictures/pois_l1_5.png){#fig-P1 height="200" width="200" fig-align="center"}

![P(10)](./Pictures/pois_l_10.png){#fig-P2 height="200" width="200" fig-align="center"}

![P(50)](./Pictures/pois_l_50.png){#fig-P3 height="200" width="200" fig-align="center"}

Poisson distributions
:::

## Gaussian distribution

[$$X\sim \mathcal{N}(\mu,\sigma^2)$$]{style="color:red"}

::: columns
::: {.column width="50%"}
-   Domain: $D_X = \mathbf{R}$

-   Density function: $f_X(x) = (2\pi \sigma^2)^{-1/2} \exp\left(-\frac{1}{2\sigma^2 }(x-\mu)^2\right)$
:::

::: {.column width="50%"}
-   Expected value: $E(X)=\mu$

-   Variance: $V(X)=\sigma^2$
:::
:::

::: {#fig-gaussian layout-ncol="3"}
![N(0,1)](./Pictures/norm_0_1_bis.png){#fig-G1 height="200" width="200" fig-align="center"}

![N(0,10)](./Pictures/norm_0_10_bis.png){#fig-G2 height="200" width="200" fig-align="center"}

![N(-100,1)](./Pictures/norm_100_1_bis.png){#fig-G3 height="200" width="200" fig-align="center"}

Gaussian densities, *dotted curve:* $\mathcal{N}(0,1)$
:::

# Joint distributions

## Independent random variables

**Idea:** (⚠️ not a rigorous definition)

Two random variables $X_1$ and $X_2$ are [independent]{style="color:red"} when knowing the value taken by $X_1$ gives no information about the value that will be taken by $X_2$ and vice versa.

**Examples** $\ldots$

## Covariance

-   Measures the strength of the relationship between two (quantitative) variables.

-   Consider two r.v. $X_1$ and $X_2$. The [covariance]{style="color:red"} between $X_1$ and $X_2$ is defined by: $$
    cov(X_1,X_2) = E[(X_1-E(X_1))(X_2-E(X_2))] = E(X_1 X_2)- E(X_1)E(X_2)
    $$

. . .

**Rules**

1.  If two r.v. are independent, then their covariance is zero. But the converse is false!

2.  $cov(X,X) = V(X)$

3.  $cov(aX+bY,cZ+dT) = ac cov(X,Z) + ad cov(X,T) + bc cov(Y,Z) + bd cov(Y,T)$

4.  $V(X_1 + X_2) = V(X_1) + V(X_2) + 2cov(X_1,X_2)$

Consequence: if $X_1$ and $X_2$ are independent, then $V(X_1 + X_2) = V(X_1) + V(X_2)$

## Linear correlation coefficient

Consider two (quantitative) r.v. $X_1$ and $X_2$.

The [linear correlation coefficient]{style="color:red"} between $X_1$ and $X_2$ is denoted by: $$
    \rho(X_1, X_2) = \frac{cov(X_1, X_2)}{\sqrt{V(X_1)}\sqrt{V(X_2)}}
    $$

. . .

Remarks :

-   $-1 \leq \rho(X_1, X_2) \leq 1$.

-   $\rho(X_1, X_2)$ only characterizes the [linear]{style="color:red"} relationship between $X_1$ and $X_2$.

✏️ Graphical considerations $\ldots$

## Joint distributions

**Discrete r.v.**

[Joint distribution:]{style="color:blue"} set of values $p_{ij}$ defined as: $$p_{ij} = P(X=x_i, Y=y_j)$$ *Remark*: $\sum_{i,j} p_{ij} = 1$

If $X$ and $Y$ are [independent]{style="color:blue"}, then $$P(X=x_i  \cap Y=y_j) = P(X=x_i) P(Y=y_j)$$

. . .

**Continuous r.v.**

If $X$ and $Y$ are [independent]{style="color:blue"}, then $$f(x,y) = h(x) g(y)$$

## Central limit theorem

Suppose $X_{1},\ldots ,X_{n}$ is a sequence of [i.i.d.]{style="color:red"} random variables with $\mathbb{E}[X_{i}]=\mu$ and $V[X_{i}]=\sigma ^{2}<\infty$. Then as $n$ approaches infinity, the random variables $\sqrt {n}({\bar {X}}_{n}-\mu )$ converge in distribution to a normal $\mathcal {N}(0,\sigma ^{2})$: [$${{\sqrt {n}}\left({\bar {X}}_{n}-\mu \right)\ \xrightarrow {d} \ {\mathcal {N}}\left(0,\sigma ^{2}\right).}$$]{style="color:red"}

# Statistical inference

## Outline

The goal is to introduce tools to draw conclusions about a population from observations collected on a subset of individuals in that population (sample)[^1].

[^1]: Image from https://commons.wikimedia.org/wiki/File:01fig-inference.png?uselang=fr

::: columns
::: {.column width="50%"}
![](./Pictures/inference.png){fig-align="center" height="250" width="300"}
:::

::: {.column width="50%"}
1.  [Estimation.]{style="color:red"}

2.  [Confidence intervals.]{style="color:red"}

3.  [Statistical tests.]{style="color:red"}
:::
:::

This requires the definition of a [model]{style="color:red"}, i.e. the specification of a random distribution for the data, and possibly an equation linking the [parameters of the distribution]{style="color:red"}[^2] of the variable to be explained to the explanatory variables.

[^2]: [The model parameters are the quantities to be estimated and tested.]{style="color:red"}

## Examples

▶️ **We are interested in estimating the proportion of patients experiencing side effects after taking a new medication.**

. . .

*Denote by* $X$ the number of patients among those of a $n$-sample that experience a side effect. We can assume that $$X \sim \mathcal{B}(n,p)$$

. . .

[$p$ is unknown and represents the probability to experience a side effect]{style="color:red"}. *The* $n$-sample is used to draw conclusions about $p$.

. . .

▶️ **We are interested in estimating the average cholesterol level in the population of people suffering from a given disease.**

. . .

*Denote by* $(X_i)_{1 \leq i \leq n}$ the cholesterol levels for each individual in a $n$-sample.

*It can be assumed that* $(X_i)_{1 \leq i \leq n}$ are mutually independant such that:

$$X_i \sim \mathcal{N}(\mu,\sigma^2)$$

$\mu$ and $\sigma^2$ are unknown. [$\mu$ represents the average cholesterol level in the population of interest.]{style="color:red"}. The $n$-sample is used to draw conclusions about $\mu$.

## Prerequisites

**Gaussian variables manipulation**

-   [Scaling]{style="color:blue"} : Consider $X \sim \mathcal{N}(\mu,\sigma^2)$ and define $Z=\frac{X-\mu}{\sigma}$. Then, $$Z \sim \mathcal{N}(0,1)$$

-   [Sum of independent gaussian r.v.'s]{style="color:blue"} : Consider $X_i \sim \mathcal{N}(\mu_i,\sigma_i^2)$, $i=1,\ldots,n$ **independent** and define $Z=\sum_{i} X_i$. Then, $$Z \sim \mathcal{N}(\sum_{i}\mu_i,\sum_{i}\sigma_i^2)$$

## Estimator, estimation

<!-- - An **estimation** is a value calculated on a sample and which we hope is a good approximation of the value we would have calculated on the total population. It is derived from an estimator.  -->

-   An **estimator** $T_n$ of a parameter is a random variable defined as a function of the random variables from the statistical model, *i.e.* $$T_n = f(X_1,\ldots,X_n)$$ An estimator is a random variable and is therefore characterized by a probability distribution.

-   The **estimation** is a realization of the random variable $T_n$ calculated from the observations in the sample: $$t_n = f(x_1,\ldots,x_n)$$

## A simple example

Let's consider a Gaussian model, *ie* $X_i \underset{i.i.d.}{\sim} \mathcal{N}(\mu,\sigma^2)$[^3] and $$\bar{X_n}=\dfrac{1}{n}\sum\limits_{i=1}^{n} X_i$$

[^3]: Think, for example, of the model for cholesterol levels in the previous slide.

1.  Is $\bar{X_n}$ a random variable or a constant?

2.  What are the distribution, the expected value and the variance of $\bar{X_n}$?

3.  Why would $\bar{X_n}$ be a good estimator for $\mu$?

**Desired properties of an estimator: unbiased, small variance,...**

## How are estimators defined in practice?

-   The [maximum likelihood estimation (MLE)]{style="color:red"} method can be used to estimate the parameters of a model[^4].

-   The idea is to find the value of the unknown parameter(s) that maximize the probability (*i.e.* the likelihood) of the data being observed (given the model).

[^4]: Many other estimation methods exist, such as the least squares method for example.

📝 *Example:* Derivation of $\bar{X_n}$ as the maximum likelihood estimator of $\mu$ in the Gaussian model.

## Confidence intervals

-   An estimation is not the true parameter value! We can only hope that the estimated value is close to the true parameter value.

-   Due to sampling variability, the estimation of a given parameter varies from sample to sample.

-   A **confidence interval** is a range of values that contains the true parameter value with a given probability $1-\alpha$. $1-\alpha$ is called the confidence level.

## Building confidence intervals

These items describe the generic steps in the construction of a confidence interval. We explain them here in the Gaussian model.

1.  Transform the estimator into a **pivotal statistics** $T_n$ whose distribution does not depend on the unknown parameters (*i.e.* is entirely known). Think about centering and standardizing $\bar{X}_n$.

2.  Represent the distribution of $T_n$ and place the quantiles of orders $\alpha/2$ and $1-\alpha/2$ ($u_{\alpha/2}$ and $u_{1-\alpha/2}$) on this representation.

3.  What is the value of $P(u_{\alpha/2} \leq T_n \leq u_{1-\alpha/2})$?

4.  Deduce $A_n$ and $B_n$ such that $P(A_n \leq \mu \leq B_n)$.

$[A_n,B_n]$ is called **probability interval**. The **confidence interval** is the realization of this probability interval calculated from the sample.

<!-- ## Back to the example -->

<!-- Assume the value of $\sigma$ is known and that $\sigma=83$. -->

<!-- Derive a confidence interval at confidence level $95\%$ of the average cholesterol level in the population. -->

<!-- ```{r comparesummary1ter} -->

<!-- summary(data$cholesterol) -->

<!-- ``` -->

<!-- You can use the values below -->

<!-- ```{r normquantiles,echo=T} -->

<!-- qnorm(0.025) -->

<!-- qnorm(0.05) -->

<!-- ``` -->

## What if $\sigma^2$ is not known?

## Important probability distributions and results

[Chi-square distribution]{style="color:blue"} : Let $X_1$, $X_2$, ..., $X_n$ be $n$ independent r.v. and identically distributed according $\mathcal{N}(0,1)$, and let [$U = X_1^2 +X_2^2 +...+X_n^2$]{style="color:red"}.

::: columns
::: {.column width="50%"}
Then [$$U \sim \chi^2(n)$$]{style="color:red"}
:::

::: {.column width="50%"}
![$\chi^2(10)$](./Pictures/chisq_n10.png){#fig-chi height="200" width="200" fig-align="left"}
:::
:::

[Important property]{style="color:blue"} : Let $X_1$, $X_2$, \ldots, $X_n$ be $n$ independent r.v. and identically distributed according $\mathcal{N}(\mu,\sigma^2)$. Then [$$\frac{1}{\sigma^2}\sum_{i=1}^{n} (X_i - \bar{X}_n)^2 \sim \chi^2(n-1)$$]{style="color:red"}

## Important probability distributions and results

[Student distribution]{style="color:blue"} : Let $X \sim \mathcal{N}(0,1)$ and $U \sim \chi^2(\nu)$ independent, and let [$T = X/\sqrt{(U/\nu)}$]{style="color:red"}. Then [$$T \sim \mathcal{T}(\nu)$$]{style="color:red"}

[Fisher distribution]{style="color:blue"} : Let $U_1$ and $U_2$ be independent r.v. s.t. $U_1 \sim \chi^2(\nu_1)$ and $U_2 \sim \chi^2(\nu_2)$ , and let [$F = (U_1/\nu_1)/(U_2/\nu_2)$]{style="color:red"}. Then [$$F \sim \mathcal{F}(\nu_1,\nu_2)$$]{style="color:red"}

::: {#fig-student-fisher layout-ncol="3"}
![$\mathcal{T}(1)$](./Pictures/student1.png){#fig-st1 height="200" width="200" fig.align="left"}

![$\mathcal{T}(10)$](./Pictures/student10.png){#fig-st10 height="200" width="200" fig.align="left"}

![$\mathcal{F}(3,10)$](./Pictures/fisher_5_30.png){#fig-fisher height="200" width="200" fig.align="\"left"}

*dotted curve:* $\mathcal{N}(0,1)$
:::

## What if $\sigma^2$ is not known?

Consider $X_i \underset{i.i.d.}{\sim} \mathcal{N}(\mu,\sigma^2)$. The usual estimator for $\sigma^2$ is [$$S_{n-1}^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X_n})^2$$]{style="color:red"} It is unbiased and follows a chi-square distribution: [$$\frac{n-1}{\sigma^2} S_{n-1}^2 \sim \chi^2_{n-1}$$]{style="color:red"}

## What if $\sigma^2$ is not known?

Instead of using $T_n = \frac{\bar{X_n}-\mu}{\sigma/\sqrt{n}}$, we use $$T_n^{(2)} = \frac{\bar{X_n}-\mu}{S_{n-1}/\sqrt{n}} \sim \mathcal{T}_{n-1}$$ as a pivotal statistics.

1.  Justify that $T_n^{(2)}$ is a pivotal statistics.

2.  Repeat the steps detailed above to derive the confidence interval of $\mu$ at confidence level $1-\alpha$.

## Synthetic example

```{r data1,echo=FALSE}
data <- data.frame(
  id = 1:10,
  gender = c(0,1,0,1,0,1,0,1,0,1),
  cholesterol = c(254, 402, 288, 354, 220, 451, 405, 280, 358, 456),
  weight_kg = c(48, 98, 72, 57, 79, 63, 84, 30, 76, 65),
  height_cm = c(158, 145, 152, 148, 160, 165, 138, 142, 173, 169)
)
```

```{r data1confin, echo=T}
data
m <- mean(data$cholesterol)
s <- sd(data$cholesterol)
c(m-s/sqrt(10)*qt(0.975,9),m+s/sqrt(10)*qt(0.975,9))
t.test(data$cholesterol,conf.level=0.95)
```

## Statistical tests

-   [Statistical hypothesis]{style="color:blue"}: any statement concerning a characteristic of the population (*ie* a parameter of the model),

-   [Null hypothesis ($H_0$)]{style="color:blue"}: the hypothesis according to which the value of the parameter of interest is equal to a reference value.

-   [Alternative hypothesis ($H_1$)]{style="color:blue"}: must reflect an incompatibility with $H_0$, for example, its opposite.

    -   two-sided test: $H_0$: $\mu = \mu_0$ vs $H_1$: $\mu \neq \mu_0$

    -   one-sided test: $H_0$: $\mu = \mu_0$ vs $H_1$: $\mu > \mu_0$

    -   one-sided test : $H_0$: $\mu = \mu_0$ vs $H_1$: $\mu < \mu_0$

**The approach of statistical testing consists in choosing between** $H_0$ and $H_1$ the most probable hypothesis in view of the observations contained in the sample.

*✏️ Example.* We wonder if the average cholesterol level of people with this disease is equal to the reference value of 400. How would you formulate $H_0$ and $H_1$?

## Test statistic

✏️ Still consider $X_i \underset{i.i.d.}{\sim} \mathcal{N}(\mu,\sigma^2)$ and the following hypothesis: $$
H_0 : \mu = \mu_0 \; vs \; H_1 : \mu \neq \mu_0
$$

1.  Recall the expression for the mean estimator.

2.  What would be its distribution if $H_0$ were true?

3.  Consider $$T_n^{(2)} = \sqrt{n} \frac{\bar{X_n}-\mu_0}{S_{n-1}}.$$ What would be the distribution of $T_n^{(2)}$ if $H_0$ were true? What are the specificities of this distribution

➡️ $T_n^{(2)}$ is the **test statistic**. The construction of the decision rule that will allow to choose between $H_0$ and $H_1$ given the data is based on this random variable (rejection area, critical value, p-value).

## Decision rule

-   When performing a statistical test, **it is always assumed** that $H_0$ is true.

-   Then, a **significance level** $\alpha$ is chosen. It represents the proportion of the tests statistic's values that would be the least compatible with $H_0$ and that would lead to the decision to reject $H_0$.

✏️ Exercise:

1.  Plot the distribution of $T_n^{(2)}$ under $H_0$ and represent $\alpha$ and the rejection zone.

2.  What critical values delimit the rejection zone?

3.  State the decision rule of the test.

## Back to the example

```{r test, echo=T}
t.test(data$cholesterol, mu=400, alternative="two.sided")
```

**Discussion**

-   p-value concept

-   What would have changed if other alternative hypothesis had been considered?

. . .

```{r testbis, echo=T}
t.test(data$cholesterol, mu=400, alternative="greater")
```

## Errors and power of a test

-   type 1 error:

-   type 2 error:

-   power:

# Principal Component Analysis and data clustering

## Principal Component Analysis (PCA)


[P]{style="color:red"}rincipal [C]{style="color:red"}omponent [A]{style="color:red"}nalysis : multidimensional exploratory analysis method

#### [Objectives]{style="color:black"}

- [Represent]{style="color:blue"} the observation of $p$ variables ($p>3$) in a space of [reduced dimension]{style="color:blue"} (typically in a plane or a 3-dimensional space) by [limiting the loss of information]{style="color:blue"}, 

- [Summarize]{style="color:blue"} the information in a synthetic way: how to keep the essential information of the data?

#### [Tools]{style="color:black"}

- No prior statistical assumption

- Matrix calculation, vectors and eigenvalues

#### [Data]{style="color:black"}

[PCA can only be applied to quantitative variables observed on the same individuals.]{style="color:red"}

## Example



Cheese is made from cow's, goat's or sheep's milk. It is a source of protein, calcium, but also vitamin A (retinol) and vitamin B9 (folic acid). The objective of this study is to specify the nutritional value of the most common cheeses.

1. Which cheeses have the same nutritional profile, and which ones have different profiles? Can we highlight the main dimensions of nutritional variability of this set of cheeses?

2. Associate similar cheeses with a particular nutritional profile.


```{r dataimport}
fromages <- read.table("fromages.txt",header=T,sep="\t")
row.names(fromages) <- fromages$Fromages
str(fromages)
#head(fromages[,-1])
```

## Notations 

#### [Data]{style="color:black"}

The values taken by the [$p$ variables]{style="color:blue"} on [$n$ individuals]{style="color:blue"} are presented in a $n\times p$ table $X$:

$$X=\left[\begin{array}{cccc}
 x_{11} & x_{12} & \ldots & x_{1p} \\
 x_{21} & x_{22} & \ldots & x_{2p} \\
 \vdots & \vdots & \vdots & \vdots \\
 x_{n1} & x_{n2} & \ldots & x_{np}
\end{array}\right]$$

- $x_{ij}$ is the value taken by individual $i$ for the variable $j$.

- The line [$x_{i\cdot}=(x_{i1} , x_{i2} , \ldots , x_{ip})$]{style="color:blue"} corresponds to the values of $p$ variables for individual $i$.

- The column [$x_{\cdot j}=(x_{1j},x_{2j},\ldots,x_{nj})$]{style="color:blue"} corresponds to the values of the variable $j$ on the $n$ individuals.


#### [Mean and variance]{style="color:black"} 

- Mean of variable $j$ : $\bar{x}_{\cdot j} = \frac{1}{n} \sum\limits_{i=1}^{n} x_{ij}$

- Variance of variable $j$ : $\sigma^2_{j} = \frac{1}{n} \sum\limits_{i=1}^{n} (x_{ij}-\bar{x}_{\cdot j})^2$

## Preliminary data transformations

::: {.fragment fragment-index=1}
#### [Data transformations:]{style="color:black"}

- Centering data: $\tilde{X} = (x_{ij}-\bar{x}_{\cdot j})_{1\leq i \leq n , 1\leq j \leq p}$

- Centering and standardizing data:  $\tilde{X} = \left(\frac{x_{ij}-\bar{x}_{\cdot j}}{\sigma_j}\right)_{1\leq i \leq n , 1\leq j \leq p}$
:::

::: {.fragment fragment-index=2}
#### [Why standardizing ?]{style="color:black"}

- The variables are of different natures; for example: age, salary, number of children

- it is felt that [their respective influences]{style="color:blue"} in the analysis [should not depend on their variability]{style="color:blue"};
for example in the analysis of grades: should a subject with a high variance have a greater influence in the analysis?
::: 

::: {.fragment fragment-index=3}

**Example: Should the data be standardized?**

```{r standardizecheese, echo=TRUE}
apply(fromages[,-1],2,mean)
apply(fromages[,-1],2,sd)
```
::: 


## PCA's goals

::: {.fragment fragment-index=1}
- Extracting the essential information from a table $X$ by providing the user with [interpretation-friendly graphical representations]{style="color:blue"}.

- Data compression/summary of large datasets

- Two main aspects :

    1. an analysis of the [similarities between individuals]{style="color:blue"} 
    $\rightarrow$ characterization of the individuals with the available variables
    
    2. an analysis of the [relationships between variables]{style="color:blue"} 

::: 

::: {.fragment fragment-index=2, style="text-align:center"}
[In what follows, PCA is presented on centered and standardized data tables.]{style="color:red"}
:::

## The space (the cloud) of individuals

Geometrically, the individuals in $X$ correspond to $n$ points of $\mathbf{R}^p$.

::: columns
::: {.column width="50%"}

- [Similarity between individuals]{style="color:blue"}: euclidian distance
$$
d_{ind}(i_1,i_2) = \sqrt{\sum_{j=1}^{p} (x_{i_1j}-x_{i_2j})^2} = \mid\mid x_{i_1 \cdot} - x_{i_2 \cdot} \mid\mid_p 
$$ 

- [Inertia]{style="color:blue"} $\rightarrow$ variability between individuals
$$
\begin{eqnarray*}
I_{ind} & = & \frac{1}{n} \sum_{j=1}^{p}\sum_{i=1}^{n} (x_{ij}-\bar{x}_{\cdot j})^2\\
& = & \frac{1}{n} \sum_{i=1}^{n} \mid \mid x_{i\cdot} - g\mid \mid ^2\\
& = & \sum_{j=1}^{p} \sigma^2_{j}
\end{eqnarray*}
$$
:::
::: {.column width="50%"}
- [Center of gravity of the cloud]{style="color:blue"}:
$$
g = (\bar{x}_{\cdot 1},\ldots,\bar{x}_{\cdot p})
$$

- The variance of the $j$-th variable is given by
$$\sigma^2_{j} = \frac{1}{n} \sum\limits_{i=1}^{n} (x_{ij}-\bar{x}_{\cdot j})^2$$ 

- **Standardized case:** $I_{ind} = p$

- **Remark:** influence of each variable different in the non standardized case
:::
:::

## The space (the cloud) of variables

Geometrically, the quantitative variables in the data table $X$ correspond to $p$ points of $\mathbf{R}^n$.

- [Distance between variables]{style="color:blue"}: scalar product
$$
< x_{\cdot j_1}, x_{\cdot j_2} > = \frac{1}{n} \sum_{i=1}^{n}   x_{i j_1} x_{i j_2}
$$
$\hookrightarrow$ Expresses the [correlation]{style="color:blue"} between the two variables (reduced case). Also a cosine.

- The [distance]{style="color:blue"} between two variables is given by the norm associated with this scalar product
$$
d_{var}(j_1,j_2) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_{ij_1}-x_{ij_2})^2} = \mid\mid x_{\cdot j_1} - x_{\cdot j_2 } \mid\mid_n
$$
- [Inertia (variability) between variables]{style="color:blue"}
$$
I_{var} = \sum_{j=1}^{p} \mid\mid x_{\cdot j}\mid\mid_n^2
$$

- Remark :
$$
I_{var} = I_{ind} (= p)
$$

## Principal components and dimension reduction

::: {.fragment fragment-index=1}
- If $p > 3$, the individuals' cloud in $\mathbf{R}^p$ is not representable.

- The PCA method produces an [approximate representation]{style="color:blue"} in a low dimensional space (usually 2 or 3) by constructing from the $p$ (correlated) variables $q<p$ new variables called [principal components]{style="color:blue"}

$\Rightarrow$ in this new reduced space, the cloud is more easily represented and the analysis is easier.
:::



::: {.fragment fragment-index=2}
- The principal components ($p$ in total) are constructed by matrix algebra operations such that

    - Each principal component is [linear combination of $p$ initial variables]{style="color:blue"}

    - The principal components are [orthogonal]{style="color:blue"} to each other (*i.e.* uncorrelated)

    - Each principal component carries a certain proportion of the total inertia $\rightarrow$ minimize the loss of information
    
    - If $I_k$ is the inertia carried by $k$th principal component 

$$
I_1 > I_2 > \ldots > I_p
$$

and

$$
I_1 + I_2 + \ldots + I_p = I_T
$$
:::

## Choosing the number of components

#### [Goal:]{style="color:black"} 

Choose $q < p$ in order to keep the maximum inertia with the minimum number of variables.

#### [Several strategies:]{style="color:black"}

- [Elbow criterion]{style="color:blue"}: keep the axes whose eigenvalues are located before the drop on the eigenvalue scree.

- [Kaiser criterion]{style="color:blue"}: only axes with inertia greater than the average inertia $I_T /p$ are kept.

**Remark:** these criteria can lead to different choices

## Choosing the number of components


```{r factominer,echo=F,include=F}
library(FactoMineR)
library(factoextra)
```

```{r acpvp,echo=TRUE}
#library(FactoMineR)
acp.res <- PCA(fromages[,-1], scale.unit = TRUE,graph=F)
acp.res$eig
```

```{r eboulisvp, echo=T, fig.height=2.5, fig.width=2.5}
#library(factoextra)
fviz_eig(acp.res)
```



## Variables' representation on correlation circles

#### [Purpose:]{style="color:blue"} 

Visualize the relations

- between variables $\rightarrow$ identify groups of correlated variables

- between principal components and initial variables $\rightarrow$ interpreting the new variables resulting from the PCA


#### [How?]{style="color:blue"}

- The better a variable is represented, the closer it is to the circle.

- For two variables perfectly represented on the plane, the cosine of the angle formed by these 2 variables is equal to the correlation coefficient between these two variables.

- Similarly for the angle formed by a principal component and a variable perfectly represented on the plane.

## Representation of variables

::: columns
::: {.column width="50%"}
```{r cerclecorr, fig.height=5, fig.width=5}
fviz_pca_var(acp.res,axes=c(1,2),repel=T)
```
:::


::: {.column width="50%"}
1.  What can we say about the relationships between variables?

2.  What is the concrete meaning of the first two principal components?
:::
:::


## Representation of individuals

#### [Interpreting the position of individuals]{style="color:blue"}

- contributes to the understanding of what the principal components represent

- allows to identify groups of individuals who are similar or on the contrary are distant


#### [How?]{style="color:blue"}

- An individual is well represented on the factorial plane if the angle formed by the individual with the axes forming this plane is small (cosine squared criteria close to 1).

- The distance between two individuals on the plane [is only interpretable if they are well represented]{style="color:blue"}:

    - if two well-represented individuals are close, they are similar individuals in the initial space

    - if two well-represented individuals are distant, they are individuals who were distant in the initial space

    - if one of the two individuals is misrepresented, nothing can be said

## Representation of individuals

```{r projind, fig.height=5, fig.width=5, echo=TRUE}
fviz_pca_ind(acp.res,axes=c(1,2),repel=T)
```

## Representation of individuals


```{r, echo=TRUE}
cos2<-cbind(acp.res$ind$cos2[,1:2],rowSums(acp.res$ind$cos2[,1:2]))
colnames(cos2) <- c("Dim.1","Dim.2","Plan.1.2")
cos2
```

## Representation of individuals

```{r projind2, fig.height=5, fig.width=5, echo=TRUE}
fviz_pca_ind(acp.res,axes=c(1,2),repel=T,col.ind = "cos2")
```

<!-- # Unsupervised classification (HCA and K-means) -->

<!-- ##  -->

<!-- **Objective**: [Group $n$ individuals into $K$ groups of individuals with common characteristics]{style="color:red"}: -->

<!-- -  each group should be as homogeneous as possible -->

<!-- -  members of a class are more similar to members of the same class than to members of other classes -->

<!-- **Unsupervised classification**: -->

<!-- - Descriptive purpose -->

<!-- - [No a priori knowledge about the groups]{style="color:red"} -->

<!-- - Is it justified to assume the existence of several groups of individuals? -->

<!-- - Question of the number of groups? [$K$ unknown]{style="color:red"} -->

<!-- ## Tools -->

<!-- To classify, we need:  -->

<!-- - choose a similarity/dissimilarity measure -->

<!-- - choose a criterion to evaluate the homogeneity of the classes -->

<!-- - choose an algorithm -->

<!-- - sometimes also choose a number of classes composing the classification -->


<!-- ## Dissimilarity -->

<!-- **Definition** -->

<!-- - $d(x_{1.},x_{2.}) = d(x_{2.},x_{1.}) \geq 0$ -->

<!-- - $d(x_{1.},x_{2.}) = 0 \Rightarrow x_{1.} = x_{2.}$ -->

<!-- - **Example:** euclidian distance (the most used in practice) -->

<!-- $$ -->
<!-- d^2(x_{1.},x_{2.}) = \sum_{j=1}^{p}(x_{1j} - x_{2j})^2 -->
<!-- $$ -->

<!-- - **Reminder:** -->

<!-- - normalization of variables (context dependent, scaling issues)  -->

<!-- - **Course framework:** all variables have the same importance. -->


<!-- ## Scatter plot -->

<!-- - **Mean** of a variable: -->
<!-- $$ -->
<!-- \bar{x}_{\cdot j} = \frac{1}{n}\sum_{i=1}^{n} x_{ij} \; , \; j=1,\ldots,p -->
<!-- $$ -->

<!-- - **Center of gravity** of $X$ (overall average): -->
<!-- $$ -->
<!-- g = (\bar{x}_{\cdot 1}, \ldots, \bar{x}_{\cdot p}) -->
<!-- $$ -->

<!-- - **Inertia** of $X$: -->
<!-- $$ -->
<!-- I_T = \sum_{i=1}^{n} d^2 (x_{i.},g) -->
<!-- $$ -->


<!-- ## Decomposition of the inertia -->

<!-- For a fixed number of groups $K$: -->

<!-- $$ -->
<!-- \begin{eqnarray*} -->
<!-- I_T & = & \sum_{i=1}^{n} d^2 (x_{i.},g)\\ -->
<!-- & = & \underbrace{\sum_{k=1}^K \sum_{i \in C_k} d^2(x_{i.},g_k)}_{I_{intra}} + \underbrace{\sum_{k=1}^K n_k d^2(g_k,g)}_{I_{inter}} -->
<!-- \end{eqnarray*} -->
<!-- $$ -->


<!-- - $g_k$: barycenter of the group $C_k$ -->

<!-- - $n_k$: number of individuals in the group $C_k$ -->

<!-- - [$I_{intra}$: intra-group inertia]{style="color:red"} (dispersion of the points around their center in the different groups) -->

<!-- - [$I_{inter}$: between-group inerti]{style="color:red"}(separability of groups) -->

<!-- **Remark:** this decomposition is only valid only if the chosen dissimilarity measure is the Euclidean distance. -->

<!-- ## Objective of the classification -->

<!-- **Objective of the classification:** -->

<!-- $$ -->
<!-- I_T = I_{intra} + I_{inter} -->
<!-- $$ -->

<!-- For a given number of groups $K$, [maximize the inter-group inertia (minimize the intra-group inertia)]{style="color:red"}.  -->

<!-- **Remark:** $I_T$ is constant for a given dataset. -->

<!-- **Difficulty:** -->

<!-- - the set of solutions is not visitable (of the order of the order of $K^n$) -->

<!-- - need to use clever solution space traversal algorithms -->

<!-- ## HAC -->

<!-- - [H]{style="color:red"}ierarchical [A]{style="color:red"}scending [C]{style="color:red"}lassification -->

<!-- - [Ascending]{style="color:red"} method: agglomeration of groups -->

<!-- - [Hierarchical]{style="color:red"} method: the successive partitions are nested within each other -->

<!-- - Algorithm:  -->

<!-- **Initialization:** each individual is a group ($I_{intra} = 0$) -->

<!-- **One iteration:** merging two groups -->

<!-- **End:** the $n$ individuals form a single group ($I_{intra} = I_T$) -->

<!-- ## Choice of groups to merge -->

<!-- - The merging of two groups is associated with an [increase in intra-group variability]{style="color:red"}; it is desired that this increase be [as small as possible]{style="color:red"}. -->

<!-- - Increase of the intra-group variability induced by the merging of the $C_k$ and $C_l$ groups -->

<!-- $$ -->
<!-- \begin{eqnarray*} -->
<!--     \Delta & = & I_{intra} (C_k \cup C_l) - I_{intra} (C_k, C_l)\\ -->
<!--     & = & \frac{n_k n_l}{n_k + n_l} d^2(g_k,g_l) -->
<!--     \end{eqnarray*} -->
<!-- $$ -->
<!-- - Dissimilarity between groups (Ward's distance) -->
<!-- $$ -->
<!-- d^2(C_k,C_l)=\frac{n_k n_l}{n_k + n_l} d^2(g_k,g_l) -->
<!-- $$ -->


<!-- ## Remark -->

<!-- - Other possible dissimilarity measures? -->

<!-- 1.  -->

<!-- $$ -->
<!-- d(C_k,C_l) = \underset{x_{i.} \in C_k, x_{i'.}\in C_l}{\text{min}} \; d(x_{i.},x_{i'.}) -->
<!-- $$ -->

<!-- 2.  -->
<!-- $$ -->
<!-- d(C_k,C_l) = \underset{x_{i.} \in C_k, x_{i'.}\in C_l}{\text{max}} \; d(x_{i.},x_{i'.}) -->
<!-- $$ -->

<!-- 3. -->
<!-- $$ -->
<!-- d(C_k,C_l) = \frac{1}{n_l n_k}\sum_{x_{i.} \in C_k, x_{i'.}\in C_l}^{} \; d(x_{i.},x_{i'.}) -->
<!-- $$ -->


<!-- ## Dendrogram -->

<!-- - The partitions of the data obtained by the HAC are represented as a tree: [dendrogram]{style="color:red"}. -->

<!-- - Fusion of two elements: branch of height proportional to the percentage of inertia loss. -->

<!-- - Determination of the number of groups: "big difference of inertia". -->

<!-- ## Example -->


<!-- ```{r dendro, fig.height=5, fig.width=5} -->
<!-- tab<-scale(fromages[,-1], center=TRUE,scale=TRUE) -->
<!-- data.dist <- dist(tab,method = "euclidean") -->
<!-- data.ward <- hclust(data.dist,method="ward.D2") -->
<!-- plot(data.ward,hang=-1) -->
<!-- rect.hclust(data.ward,k=2) -->
<!-- cluster <- cutree(data.ward, k = 2) -->
<!-- ``` -->

<!-- ## Interpretation of groups -->


<!-- ```{r, fig.height=5, fig.width=5} -->
<!-- fviz_pca_ind(acp.res,axes=c(1,2),habillage=as.factor(cluster),repel=T) -->
<!-- ``` -->

<!-- ## Example -->


<!-- ```{r dendro2, fig.height=5, fig.width=5} -->
<!-- plot(data.ward,hang=-1) -->
<!-- rect.hclust(data.ward,k=3) -->
<!-- cluster <- cutree(data.ward, k = 3) -->
<!-- ``` -->

<!-- ## Interpretation of groups -->


<!-- ```{r, fig.height=5, fig.width=5} -->
<!-- fviz_pca_ind(acp.res,axes=c(1,2),habillage=as.factor(cluster),repel=T) -->
<!-- ``` -->

<!-- ## K-means method -->

<!-- - The number of groups $K$ is fixed a priori. -->

<!-- - Algorithm: -->

<!-- **Initialization:** we choose $K$ centers of groups -->
<!-- $\mu_1,\ldots,\mu_K$ among the individuals. -->

<!-- **One iteration:** -->

<!-- Each individual is classified in the group whose center is closest. -->


<!-- Definition of the new group centers as the barycenters of each group. -->

<!-- **End:** "convergence'' -->


<!-- ## Remark -->

<!-- - "Convergence": decrease of the intra-group inertia at each iteration. -->

<!-- - Faster algorithm than HAC. -->

<!-- - The result depends on the initialization. -->

<!-- - There is a wide variety of initialization modes, often random. -->


<!-- ## Example -->


<!-- ```{r kmeans} -->
<!-- res.k.means <- kmeans(fromages[,-1],centers=3) -->
<!-- res.k.means$centers -->
<!-- res.k.means$size -->
<!-- res.k.means$totss -->
<!-- res.k.means$withinss -->
<!-- res.k.means$tot.withinss -->
<!-- res.k.means$betweenss -->
<!-- ``` -->

<!-- ## Example -->

<!-- ```{r, fig.height=5, fig.width=5} -->
<!-- fviz_pca_ind(acp.res,axes=c(1,2),habillage=as.factor(res.k.means$cluster),repel=T) -->
<!-- ``` -->

<!-- ## Choosing the number of groups -->

<!-- - There is no classification that is strictly better than the others. -->

<!-- - This means measuring the quality of each competing classification and often making compromises. -->

<!-- - The interpretability of classes often plays a role. -->


<!-- ## Exemple -->

<!-- ```{r} -->
<!-- inertie.expl <- rep(0,times=10) -->
<!-- for (k in 2:10){ -->
<!--   clus <- kmeans(tab,centers=k,nstart=5) -->
<!--   inertie.expl[k] <- clus$betweenss/clus$totss } -->
<!-- plot(1:10,inertie.expl,type="b",xlab="Nb. de groupes",ylab="% inertie expliquée") -->
<!-- ``` -->

